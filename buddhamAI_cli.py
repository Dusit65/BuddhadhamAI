# buddhamAI_cli
import sys
import os
import pickle
import json
import numpy as np
import faiss
import ollama
import time
import subprocess
import traceback
import datetime
from reDocuments import reDocuments
from debugger import format_duration, log

log_file = "buddhamAI_cli.log"
required_models = ["gpt-oss:20b", "nomic-embed-text"]

def clear_screen():
    if os.name == 'nt':  # Windows
        os.system('cls')
    else:  # Unix/Linux/Mac
        os.system('clear')
        
if not os.path.exists(log_file):
        open(log_file, "w").close()
with open(log_file, "r+") as f:
    f.truncate(0)

def get_installed_models():
    # ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ --json ‡∏Å‡πà‡∏≠‡∏ô
    try:
        result = subprocess.run(
            ["ollama", "list", "--json"],
            capture_output=True,
            text=True
        )
        if result.returncode == 0 and result.stdout.strip().startswith("["):
            return [m["name"] for m in json.loads(result.stdout)]
    except Exception:
        pass  # ‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á ‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ parse ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤

    # ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏Å‡∏ï‡∏¥
    result = subprocess.run(
        ["ollama", "list"],
        capture_output=True,
        text=True
    )
    lines = result.stdout.strip().split("\n")
    models = []
    for line in lines[1:]:  # ‡∏Ç‡πâ‡∏≤‡∏° header
        parts = line.split()
        if parts:
            models.append(parts[0])
    return models

def check_and_pull_models(models_to_check):
    try:
        local_model_names = get_installed_models()
        missing_models = [m for m in models_to_check if m not in local_model_names]

        if missing_models:
            for model in missing_models:
                log(f"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• {model} ...")
                subprocess.run(["ollama", "pull", model], check=True)
                log(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î {model} ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß")
        else:
            log("‚úÖ ‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß")
    except Exception:
        log("‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î:\n" + traceback.format_exc())
        exit(1)

EMB_PATH = "embeddings.npy"
META_PATH = "metadata.pkl"
DOCS_ALL_PATH = "documents/documentsPkl/documents_all.pkl"
start = None
end = None

def load_all_documents_pickle(path=DOCS_ALL_PATH):
    if not os.path.isfile(path):
        log(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {path} ‡∏£‡∏±‡∏ô reDocuments() ‡πÅ‡∏ó‡∏ô")
        reDocuments()
        import time
        time.sleep(2)  # ‡∏£‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï
    try:
        with open(path, "rb") as f:
            docs = pickle.load(f)
        log(f"‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å {path} ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô {len(docs)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        return docs
    except Exception as e:
        log(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Ç‡∏ì‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå {path}: {e}")
        log("‡∏Å‡∏≥‡∏•‡∏±‡∏á reDocuments()")
        reDocuments()
        time.sleep(2)
        with open(path, "rb") as f:
            docs = pickle.load(f)
        return docs

def flatten_docs(raw):
    docs = []
    for book_name, chapters in raw.items():
        for chapter_key, pages in chapters.items():
            for page_key, content in pages.items():
                chapter_num = int(chapter_key.replace("chapter ", ""))
                page_num = int(page_key.replace("page ", ""))
                docs.append({
                    "bookname": book_name,
                    "chapter": chapter_num,
                    "page": page_num,
                    "content": content
                })
    return docs

def create_and_save_embeddings_and_metadata(docs):
    embeddings = []
    for doc in docs:
        content = doc.get("content", "")
        if not content:
            continue
        emb = ollama.embeddings(model='nomic-embed-text', prompt=content)['embedding']
        embeddings.append(emb)
    embeddings = np.array(embeddings, dtype='float32')
    log(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á {EMB_PATH}")
    np.save(EMB_PATH, embeddings)
    with open(META_PATH, 'wb') as f:
        pickle.dump(docs, f)

def load_embeddings_and_metadata():
    log(f"‡πÇ‡∏´‡∏•‡∏î {EMB_PATH} ‡πÅ‡∏•‡∏∞ {META_PATH}")
    embeddings = np.load(EMB_PATH)
    with open(META_PATH, 'rb') as f:
        metadata = pickle.load(f)
    return embeddings, metadata

def search(query, index, metadata, top_k, max_distance):
    log(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: {query} ‡∏î‡πâ‡∏ß‡∏¢ top_k={top_k} ‡πÅ‡∏•‡∏∞ max_distance={max_distance}")
    q_emb = ollama.embeddings(model='nomic-embed-text', prompt=query)['embedding']
    q_emb = np.array([q_emb], dtype='float32')
    distances, ids = index.search(q_emb, top_k)
    log(f"‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ nearest neighbors ‡πÄ‡∏à‡∏≠ {len(ids[0])} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
    results = []
    for i, idx in enumerate(ids[0]):
        if idx < len(metadata):
            dist = distances[0][i]
            if max_distance is None or dist <= max_distance:
                results.append((metadata[idx], dist))
                log(f"‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: index={idx}, distance={dist:.4f}")
            else:
                log(f"‡∏ï‡∏±‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: index={idx}, distance={dist:.4f} ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏Å‡∏¥‡∏ô max_distance={max_distance}")
    log(f"‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå {len(results)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà {short_references([doc for doc, _ in results])}")
    return results

def short_references(metadata):
    sorted_docs = sorted(metadata, key=lambda d: (d['bookname'], d['chapter'], d['page']))
    return ", ".join([
        f"{d['bookname']} ‡∏ö‡∏ó {d['chapter']} ‡∏´‡∏ô‡πâ‡∏≤ {d['page']}"
        for d in sorted_docs
    ])

def check_rejection_message(text: str) -> bool:
    rejection_phrases = [
        "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö...‡∏ú‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ú‡∏°‡∏ñ‡∏π‡∏Å‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏û‡∏£‡∏∞‡∏û‡∏∏‡∏ó‡∏ò‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô",
        "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö‚Ä¶‡∏ú‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‚Ä¶ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ú‡∏°‡∏ñ‡∏π‡∏Å‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏û‡∏£‡∏∞‡∏û‡∏∏‡∏ó‡∏ò‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô",
        "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö‚Ä¶‡∏ú‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ú‡∏°‡∏ñ‡∏π‡∏Å‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏û‡∏£‡∏∞‡∏û‡∏∏‡∏ó‡∏ò‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô",
        "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö...‡∏ú‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ....",
        "‡∏ú‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ",
        "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"
    ]
    return any(phrase in text for phrase in rejection_phrases)

def ask(query, index, metadata, top_k=None, max_distance=None):
    top_k=len(query) if top_k is None else top_k
    global start
    results = search(query, index, metadata, top_k, max_distance=max_distance)

    contexts = [
        f"{doc['content']}"
        for doc, _ in results
    ]
    full_context = "\n".join(contexts)
    prompt = f"""‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:\n{full_context}\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {query}"""
    model = 'gpt-oss:20b'
    log(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ñ‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•: \"{model}\" ‡∏î‡πâ‡∏ß‡∏¢ prompt:\n{prompt}")
    start = time.perf_counter()
    response = ollama.chat(
        model=model,
        messages=[
            {"role": "system", "content": "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏û‡∏£‡∏∞‡∏û‡∏ó‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ ‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö...‡∏ú‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ....‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ú‡∏°‡∏ñ‡∏π‡∏Å‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏û‡∏£‡∏∞‡∏û‡∏∏‡∏ó‡∏ò‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"},
            {"role": "user", "content": prompt}
        ]
    )
    answer = response['message']['content']
    ref_text = short_references([doc for doc, _ in results])
    end = time.perf_counter()
    processing_time = format_duration(end - start)
    log(f"‡∏ñ‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏• \"{model}\" ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ {processing_time}")
    if not check_rejection_message(answer):
        return {
            "answer": answer,
            "references": ref_text,
            "duration": processing_time
        }
    else:
        return {
            "answer": answer,
            "references": "",
            "duration": 0
        }
    
def init_bot():
    documents_raw = load_all_documents_pickle()
    documents = flatten_docs(documents_raw)
    if not (os.path.exists(EMB_PATH) and os.path.exists(META_PATH)):
        create_and_save_embeddings_and_metadata(documents)
    embeddings, metadata = load_embeddings_and_metadata()
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    return index, metadata

def ask_cli():
    log("‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô BuddhamAI")
    check_and_pull_models(required_models)
    # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (argument ‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà flag)
    message = None
    top_k = None
    max_distance = None

    # ‡∏ß‡∏ô‡∏≠‡πà‡∏≤‡∏ô argv ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà sys.argv[1]
    args = sys.argv[1:]
    i = 0
    while i < len(args):
        arg = args[i]
        if arg == '-k' and i + 1 < len(args):
            try:
                top_k = int(args[i+1])
            except ValueError:
                top_k = None
            i += 2
        elif arg == '-d' and i + 1 < len(args):
            try:
                max_distance = float(args[i+1])
            except ValueError:
                max_distance = None
            i += 2
        elif not arg.startswith('-') and message is None:
            message = arg
            i += 1
        else:
            i += 1

    if message is None or message.strip() == "":
        result = {"rejected": True, "answer": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°", "references": "‡πÑ‡∏°‡πà‡∏°‡∏µ", "duration": 0}
        data = {"data": result}
        json_str = json.dumps(data, ensure_ascii=False)
        log(json_str)
        print(json_str)
        sys.exit(0)

    # if top_k is None or top_k <= 0:
    #     top_k = 3  # default

    index, metadata = init_bot()
    result = ask(message, index, metadata, top_k=top_k, max_distance=max_distance)

    data = {"data": result}
    json_str = json.dumps(data, ensure_ascii=False)
    log(json_str)
    print(json_str)
    sys.exit(0)

if __name__ == "__main__":
    ask_cli()